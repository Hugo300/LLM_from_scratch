{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48dbd25",
   "metadata": {},
   "source": [
    "## Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b8f202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in terminal open \"ollama serve\" to start the app and the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c532b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "\n",
    "    return running\n",
    "\n",
    "# check ollama is running\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama is not running. Launch it before using this notebook.\")\n",
    "\n",
    "print(f\"Ollama running: {ollama_running}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55a9b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predictions stored\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"./instruction_data/instruction-data-with-model-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b98f8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is based on a older, more widespread approach\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input\\n{entry['input']}\" if entry['input'] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7aaefac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a subjective question! The concept of \"best\" can vary greatly depending on individual perspectives, values, and experiences. It's also important to recognize that everyone has their own unique strengths, weaknesses, and contributions to make.\n",
      "\n",
      "Rather than trying to identify a single person as the \"best,\" I'd like to offer some alternative perspectives:\n",
      "\n",
      "1. **Every person is special**: Each individual has their own unique qualities, talents, and experiences that make them valuable and important in their own way.\n",
      "2. **There's no one-size-fits-all answer**: What might be considered the \"best\" by one person might not be the same for another. Different people have different values, priorities, and perspectives.\n",
      "3. **We can learn from each other**: Rather than trying to identify a single \"best\" person, we can learn from and appreciate the diverse range of individuals who make our world more interesting and enriching.\n",
      "\n",
      "So, instead of focusing on a single person, let's celebrate the diversity and individuality that makes our world so rich and fascinating!\n",
      "\n",
      "Who do you think is an inspiring or remarkable person?\n"
     ]
    }
   ],
   "source": [
    "# query ollama\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, model=\"llama3.2\", url=\"http://localhost:11434/api/chat\"):\n",
    "\n",
    "    # create api payload\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"options\": {\n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    # send request and process response\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "        \n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "        \n",
    "    return response_data\n",
    "\n",
    "\n",
    "# test api call\n",
    "model = \"llama3.2\"\n",
    "result = query_model(\"Who is the best person in the world?\", model=model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970d235",
   "metadata": {},
   "source": [
    "### Scoring our models response using another llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a83aa42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "\tThe car is very fast.\n",
      "\n",
      "Expected Response: \n",
      "\tThe car is as fast as lightning.\n",
      "Model Response: \n",
      "\tThe car is as fast as a bullet.\n",
      "Score: \n",
      "\tTo rewrite the sentence \"The car is very fast\" using a simile, we can replace \"very fast\" with a comparative phrase that uses \"like\" or \"as.\" Here's an example:\n",
      "\n",
      "\"The car is as fast as lightning.\"\n",
      "\n",
      "Now, let's evaluate the model response \"The car is as fast as a bullet.\" on a scale from 0 to 100.\n",
      "\n",
      "Score: 80\n",
      "\n",
      "This response is close to the correct answer, but it uses a metaphor (comparing the car to a bullet) instead of a simile. A simile explicitly compares two things using \"like\" or \"as,\" whereas a metaphor states that one thing is another thing. While the response is still clear and effective, it doesn't meet the specific request for a simile.\n",
      "\n",
      "-------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:1]:\n",
    "    prompt = (\n",
    "        f\"Given the input '{format_input(entry)}' \"\n",
    "        f\"and the correct output '{entry['output']}', \"\n",
    "        f\"score the model response '{entry['model_response']}'\"\n",
    "        \" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "\n",
    "    print(f\"Input: \\n\\t{entry['input']}\")\n",
    "    print(f\"\\nExpected Response: \\n\\t{entry['output']}\")\n",
    "    print(f\"Model Response: \\n\\t{entry['model_response']}\")\n",
    "    print(f\"Score: \\n\\t{query_model(prompt)}\")\n",
    "    print(\"\\n-------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70a8891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring:   9%|▉         | 10/110 [00:21<03:50,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 60\n",
      "\n",
      "The model's response contains incorrect information about prime and composite numbers. The correct classification should be:\n",
      "\n",
      "Prime numbers: 11, 19\n",
      "Composite numbers: 14\n",
      "\n",
      "The model incorrectly included numbers like 3, 5, 9, 11, 13, 15, 17 in the list of composite numbers, which are actually prime numbers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring:  22%|██▏       | 24/110 [00:51<03:11,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 60\n",
      "\n",
      "Explanation: The correct categorization should be \"Fish: Shark, Trout\" and \"Mammals: Dolphin\". The model response \"Dolphin, Trout\" incorrectly places both animals in the same category.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring:  29%|██▉       | 32/110 [01:09<02:52,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I'd rewrite the sentence as: \"This task is straightforward.\"\n",
      "\n",
      "As for scoring my previous response, I'd give it a 0 out of 100.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring:  62%|██████▏   | 68/110 [02:26<01:37,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I can split the sentence into two declarative sentences: 'The movie was long.' and 'It was interesting.'\n",
      "\n",
      "However, I will not use this response as it does not match the expected output.\n",
      "\n",
      "Here's another attempt:\n",
      "\n",
      "1. The movie was long.\n",
      "2. It was interesting.\n",
      "\n",
      "As for scoring the model response 'and The movie was interesting.', I would give it a score of 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring:  68%|██████▊   | 75/110 [02:41<01:21,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: I would rate this response a 0 out of 100.\n",
      "\n",
      "The model's response does not sort the list in descending order as requested. Instead, it generates a large amount of random numbers that do not resemble any meaningful output. The correct sorted list should be: 25, 16, 10, 7, 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████| 110/110 [03:56<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 105 of 110\n",
      "Average score: 39.16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3.2\"):\n",
    "    scores = []\n",
    "\n",
    "    for entry in tqdm(json_data, desc=\"Scoring\"):\n",
    "        prompt = (\n",
    "            f\"Given the input '{format_input(entry)}' \"\n",
    "            f\"and the correct output '{entry['output']}', \"\n",
    "            f\"score the model response '{entry['model_response']}'\"\n",
    "            \" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            \"Respond with the integer number only.\"\n",
    "        )\n",
    "\n",
    "        score = query_model(prompt, model)\n",
    "\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
